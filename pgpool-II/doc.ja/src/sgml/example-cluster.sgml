<!-- doc/src/sgml/example-cluster.sgml -->

<sect1 id="example-cluster">
 <title><productname>Pgpool-II</productname> + Watchdogの構築の例</title>
 <para>
  ここでは、ストリーミングレプリケーション構成の<productname>PostgreSQL</productname>を<productname>Pgpool-II</productname>で管理するシステムの構成例を示します。この例では、3台の<productname>Pgpool-II</productname>を使って<productname>PostgreSQL</productname>を管理し、単一障害点やスプリットブレインの起きない堅牢なクラスタを運用することが可能です。
 </para>
 <para>
  この設定例では<productname>PostgreSQL</productname> 11を使っていますが、各種スクリプトは<productname>PostgreSQL</productname> 12でも動作確認を行っています。
 </para>
 <sect2 id="example-cluster-requirement">
  <title>前提条件</title>
  <para>
   <productname>Pgpool-II</productname>サーバと<productname>PostgreSQL</productname>サーバが同じサブネットにあることを前提とします。
  </para>
 </sect2>

 <sect2 id="example-cluster-structure">
  <title>全体構成</title>
  <para>
   今回は、Linuxサーバを3台用意し、それぞれのホスト名は 「server1」、「server2」、「server3」 とします。使用するOSはすべてCentOS 7.4とします。それぞれのサーバに<productname>PostgreSQL</productname>と<productname>Pgpool-II</productname>をインストールします。3台の<productname>PostgreSQL</productname>がストリーミングレプリケーション構成になります。全体構成図は以下の通りです。
  </para>
  <para>
   <figure>
    <title>全体構成図</title>
    <mediaobject>
     <imageobject>
      <imagedata fileref="cluster_40.gif">
     </imageobject>
    </mediaobject>
   </figure>
  </para>
  <note>
   <para>
    「アクティブ」「スタンバイ」「Primary」「Standby」といった役割は固定されているものではなく、運用と共に変化することがあります。
   </para>
  </note>

  <table id="example-cluster-table-ip">
   <title>ホスト名とIPアドレス</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>ホスト名</entry>
      <entry>IPアドバイス</entry>
      <entry>仮想IP</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>server1</entry>
      <entry>192.168.137.101</entry>
      <entry morerows="2">192.168.137.150</entry>
     </row>
     <row>
      <entry>server2</entry>
      <entry>192.168.137.102</entry>
     </row>
     <row>
      <entry>server3</entry>
      <entry>192.168.137.103</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <table id="example-cluster-table-postgresql-config">
   <title>PostgreSQLのバージョンと設定情報</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>項目</entry>
      <entry>値</entry>
      <entry>説明</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>PostgreSQLバージョン</entry>
      <entry>11.1</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>ポート番号</entry>
      <entry>5432</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>$PGDATA</entry>
      <entry>/var/lib/pgsql/11/data</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>アーカイブモード</entry>
      <entry>有効</entry>
      <entry>/var/lib/pgsql/archivedir</entry>
     </row>
     <row>
      <entry>レプリケーションスロット</entry>
      <entry>有効</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>自動起動</entry>
      <entry>自動起動しない</entry>
      <entry>-</entry>
     </row>
    </tbody>
   </tgroup>
  </table>


  <table id="example-cluster-table-pgpool-config">
   <title>Pgpool-IIのバージョンと設定情報</title>
   <tgroup cols="3">
    <thead>
     <row>
      <entry>項目</entry>
      <entry>値</entry>
      <entry>説明</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>Pgpool-IIバージョン</entry>
      <entry>4.1</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry morerows='3'>ポート番号</entry>
      <entry>9999</entry>
      <entry>Pgpool-IIが接続を受け付けるポート番号</entry>
     </row>
     <row>
      <entry>9898</entry>
      <entry>PCPプロセスが接続を受け付けるポート番号</entry>
     </row>
     <row>
      <entry>9000</entry>
      <entry>watchdogが接続を受け付けるポート番号</entry>
     </row>
     <row>
      <entry>9694</entry>
      <entry>Watchdogのハートビート信号を受信するUDPポート番号</entry>
     </row>
     <row>
      <entry>設定ファイル</entry>
      <entry>/etc/pgpool-II/pgpool.conf</entry>
      <entry>Pgpool-IIの設定ファイル</entry>
     </row>
     <row>
      <entry>Pgpool-II起動ユーザ</entry>
      <entry>postgres (Pgpool-II 4.1以降)</entry>
	  <entry>Pgpool-II 4.0 以前のバージョンでは、デフォルトではrootでPgpool-IIを起動する</entry>
     </row>
     <row>
      <entry>Pgpool-II動作モード</entry>
      <entry>ストリーミングレプリケーションモード</entry>
      <entry>-</entry>
     </row>
     <row>
      <entry>Watchdog機能</entry>
      <entry>有効</entry>
      <entry>ハードビート方式</entry>
     </row>
     <row>
      <entry>自動起動</entry>
      <entry>自動起動しない</entry>
      <entry>-</entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect2>

 <sect2 id="example-cluster-installation">
  <title>インストール</title>
  <para>
   すべてのサーバに<productname>PostgreSQL</productname> 11.1と<productname>Pgpool-II</productname> 4.1をRPMからインストールします。
  </para>
  <para>
   <productname>PostgreSQL</productname>のインストールは<productname>PostgreSQL</productname>コミュニティのリポジトリを使います。
  </para>
  <programlisting>
   # yum install https://download.postgresql.org/pub/repos/yum/11/redhat/rhel-7-x86_64/pgdg-centos11-11-2.noarch.rpm
   # yum install postgresql11 postgresql11-libs postgresql11-devel postgresql11-server
  </programlisting>

  <para>
   <productname>Pgpool-II</productname>のインストールは<productname>Pgpool-II</productname>開発コミュニティが提供するYumリポジトリを用いてインストールします。
  </para>
  <programlisting>
   # yum install http://www.pgpool.net/yum/rpms/4.1/redhat/rhel-7-x86_64/pgpool-II-release-4.1-1.noarch.rpm
   # yum install pgpool-II-pg11-*
  </programlisting>
 </sect2>

 <sect2 id="example-cluster-pre-setup">
  <title>事前設定</title>

  <itemizedlist>
   <listitem>
    <para>
     <productname>PostgreSQL</productname>プライマリサーバのみでストリーミングレプリケーションの設定を行います。設定方法についてはここでは省略します。
     スタンバイサーバの設定は、プライマリが起動した状態で、<productname>Pgpool-II</productname>のオンラインリカバリ機能を使って行います。
    </para>
   </listitem>
  </itemizedlist>
  <itemizedlist>
   <listitem>
    <para>
     この設定の例ではアーカイブリカバリを行うように設定します。
    </para>
    <para>
     まず、すべてのサーバにて<acronym>WAL</acronym>を格納するディレクトリ<filename>/var/lib/pgsql/archivedir</filename>を事前に作成します。この設定例では、Primaryサーバのみで<acronym>WAL</acronym>アーカイブをローカルで実施します。
    </para>
    <programlisting>
     [全サーバ]# su - postgres
     [全サーバ]$ mkdir /var/lib/pgsql/archivedir
    </programlisting>

    <para>
     次に<literal>server1</literal>にて、設定ファイル<filename>$PGDATA/postgresql.conf</filename>を以下のように編集します。
     <literal>pg_rewind</literal>を使うために<literal>wal_log_hints</literal>を有効にしておきます。
     プライマリが後でスタンバイになる可能性があるので、<varname>hot_standby = on</varname>にしておきます。

    </para>
    <programlisting>
     listen_addresses = '*'
     archive_mode = on
     archive_command = 'cp "%p" "/var/lib/pgsql/archivedir/%f"'
     max_wal_senders = 10
     max_replication_slots = 10
     wal_level = replica
     hot_standby = on
     wal_log_hints = on 
    </programlisting>
   </listitem>

   <listitem>
    <para>
     Pgpool-IIのヘルスチェック及びレプリケーションの遅延チェックでPostgreSQLのユーザを設定する必要があります。セキュリティ上の理由で、この設定例ではスーパーユーザを使わないようにします。
     <productname>Pgpool-II</productname>のレプリケーションの遅延チェックとヘルスチェック用のユーザ<literal>pgpool</literal>を作成します。
     また、<productname>PostgreSQL</productname>プライマリサーバ<literal>server1</literal>でレプリケーション専用ユーザ<literal>repl</literal>を作成します。
     <productname>Pgpool-II</productname>4.0からSCRAM認証を利用できるようになりました。この設定例では、<literal>scram-sha-256</literal>認証方式を利用します。
     まず、<literal>password_encryption = 'scram-sha-256'</literal>に変更してから、ユーザを登録します。
    </para>

    <table id="example-cluster-user">
     <title>ユーザ</title>
     <tgroup cols="3">
      <thead>
       <row>
        <entry>ユーザ名</entry>
        <entry>パスワード</entry>
        <entry>備考</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>repl</entry>
        <entry>repl</entry>
        <entry>PostgreSQLのレプリケーション専用ユーザ</entry>
       </row>
       <row>
        <entry>pgpool</entry>
        <entry>pgpool</entry>
        <entry>Pgpool-IIのレプリケーション遅延チェック、ヘルスチェック専用ユーザ</entry>
       </row>
       <row>
        <entry>postgres</entry>
        <entry>postgres</entry>
        <entry>オンラインリカバリを実行するユーザ</entry>
       </row>
      </tbody>
     </tgroup>
    </table>

    <programlisting>
     [server1]# psql -U postgres -p 5432
     postgres=# SET password_encryption = 'scram-sha-256';
     postgres=# CREATE ROLE pgpool WITH LOGIN;
     postgres=# CREATE ROLE repl WITH REPLICATION LOGIN;
     postgres=# \password pgpool
     postgres=# \password repl
     postgres=# \password postgres
    </programlisting>

    <para>
     <xref linkend="SQL-SHOW-POOL-NODES">コマンドでレプリケーション状態と同期レプリケーション状態を表示するには、<literal>pgpool</literal>ユーザにデフォルトロール<literal>pg_monitor</literal>を付与する必要があります (<productname>Pgpool-II</productname>4.1以降)。
      以下のコマンドで<literal>pgpool</literal>ユーザをそのグループに所属させます。
    </para>
    <programlisting>
     GRANT pg_monitor TO pgpool;
    </programlisting>

    <note>
     <para>
      <xref linkend="guc-detach-false-primary">(<productname>Pgpool-II</productname> 4.1以降)を利用する予定がある場合、"pgpool" ロールは<productname>PostgreSQL</productname>のスーパーユーザーであるか、<literal>pg_monitor</literal>グループに所属する必要があります。
     </para>
    </note>

    <para>
     <productname>Pgpool-II</productname>サーバと<productname>PostgreSQL</productname>バックエンドサーバが同じサブネットワークにあることを想定し、各ユーザが<literal>scram-sha-256</literal>認証方式で接続できるように、<filename>pg_hba.conf</filename>を編集しておきます。
    </para>
    <programlisting>
     host    all             all             samenet                 scram-sha-256
     host    replication     all             samenet                 scram-sha-256
    </programlisting>
   </listitem>

   <listitem>
    <para>
     自動フェイルオーバ、オンラインリカバリ機能を利用するには、<productname>Pgpool-II</productname>起動ユーザ(デフォルトでは<literal>root</literal>)と<literal>postgres</literal>ユーザ間、<literal>postgres</literal>ユーザと<literal>postgres</literal>ユーザ間が双方向に<emphasis>パスワードなし</emphasis>で<literal>SSH</literal>接続できる状態になっている必要があります。全サーバで以下のコマンドを実行し、<literal>SSH</literal>の設定を行います。生成される鍵ファイル名は<literal>id_rsa_pgpool</literal>とします。
    </para>
    <programlisting>
     [全サーバ]# cd ~/.ssh
     [全サーバ]# ssh-keygen -t rsa -f id_rsa_pgpool
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3

     [全サーバ]# su - postgres
     [全サーバ]$ cd ~/.ssh
     [全サーバ]$ ssh-keygen -t rsa -f id_rsa_pgpool
     [全サーバ]$ ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
     [全サーバ]# ssh-copy-id -i id_rsa_pgpool.pub postgres@server3
    </programlisting>
    <para>
     設定後、<literal>root</literal>ユーザ及び<literal>postgres</literal>ユーザから<command>ssh postgres@serverX -i ~/.ssh/id_rsa_pgpool</command>コマンドを実行し、パスワード入力せずログインできることを確認してください。
     必要に応じて<filename>/etc/ssh/sshd_config</filename>を編集し、sshdを再起動してください。
    </para>
   </listitem>

   <listitem>
    <para>
     <literal>repl</literal>ユーザのパスワード入力なしで、ストリーミングレプリケーションとオンラインリカバリを行うために、または<literal>postgres</literal>ユーザで<application>pg_rewind</application>を実行するために、すべてのサーバにて<literal>postgres</literal>ユーザのホームディレクト<filename>/var/lib/pgsql</filename> に<filename>.pgpass</filename>を作成・配置し、パーミッションを 600 に設定しておきます。
    </para>
    <programlisting>
     [全サーバ]# su - postgres
     [全サーバ]$ vi /var/lib/pgsql/.pgpass
     (以下を追加)
     server1:5432:replication:repl:&lt;replユーザのパスワード&gt;
     server2:5432:replication:repl:&lt;replユーザのパスワード&gt;
     server3:5432:replication:repl:&lt;replユーザのパスワード&gt;
     server1:5432:postgres:postgres:&lt;postgresユーザのパスワード&gt;
     server2:5432:postgres:postgres:&lt;postgresユーザのパスワード&gt;
     server3:5432:postgres:postgres:&lt;postgresユーザのパスワード&gt;
     [全サーバ]$ chmod 600  /var/lib/pgsql/.pgpass
    </programlisting>
   </listitem>

   <listitem>
    <para>
     <productname>Pgpool-II</productname>や<productname>PostgreSQL</productname>に接続する際には、ファイアーウォールによって目的のポートが開けられていなければなりません。<systemitem>CentOS/RHEL7</systemitem>の場合、以下のように設定します。
    </para>
    <programlisting>
     [全サーバ]# firewall-cmd --permanent --zone=public --add-service=postgresql
     [全サーバ]# firewall-cmd --permanent --zone=public --add-port=9999/tcp --add-port=9898/tcp --add-port=9000/tcp  --add-port=9694/udp
     [全サーバ]# firewall-cmd --reload
    </programlisting>
   </listitem>

  </itemizedlist>
 </sect2>

 <sect2 id="example-cluster-pgpool-config">
  <title><productname>Pgpool-II</productname>の設定</title>
  <sect3 id="example-cluster-pgpool-config-common">
   <title>共通設定</title>
   <para>
    以下の操作は<literal>server1</literal>, <literal>server2</literal>, <literal>server3</literal>での共通の設定です。
   </para>
   <para>
    RPMからインストールした場合、すべての<productname>Pgpool-II</productname>の設定ファイルは<filename>/etc/pgpool-II</filename>にあります。今回はストリーミングレプリケーションモードのテンプレートとして<filename>pgpool.conf.sample-stream</filename>サンプルファイルを使用します。
   </para>
   <programlisting>
    [全サーバ]# cp -p /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
   </programlisting>
   <para>
    <productname>Pgpool-II</productname>が全てのIPアドレスから接続を受け付けるように、<xref linkend="GUC-LISTEN-ADDRESSES">パラメータに<literal>'*'</literal>を設定します。
   </para>
   <programlisting>
    listen_addresses = '*'
   </programlisting>
   <para>
    レプリケーションの遅延チェックユーザ<xref linkend="GUC-SR-CHECK-USER">にpgpoolユーザを設定します。
     この設定例では、<xref linkend="GUC-SR-CHECK-PASSWORD">は<filename>pgpool.conf</filename>に指定せず、<xref linkend="GUC-POOL-PASSWD">ファイルに作成します。<productname>Pgpool-II</productname> 4.0から、<xref linkend="GUC-SR-CHECK-PASSWORD">が空白の場合、<productname>Pgpool-II</productname>は空のパスワードを使用する前にまず<xref linkend="GUC-POOL-PASSWD">ファイルから<xref linkend="GUC-SR-CHECK-USER">に指定したユーザのパスワードを取得できるか試みます。
   </para>
   <programlisting>
    sr_check_user = 'pgpool'
    sr_check_password = ''
   </programlisting>
   <para>
    自動フェイルオーバのため、ヘルスチェックを有効にします。<xref linkend="GUC-HEALTH-CHECK-PERIOD">のデフォルト値が0で、これはヘルスチェックが無効であることを意味します。
     また、ネットワークが不安定な場合には、バックエンドが正常であるにも関わらず、ヘルスチェックに失敗し、フェイルオーバや縮退運転が発生してしまう可能性があります。そのようなヘルスチェックの誤検知を防止するため、ヘルスチェックのリトライ回数を<varname>health_check_max_retries = 3</varname> に設定しておきます。
     <xref linkend="GUC-HEALTH-CHECK-USER">、<xref linkend="GUC-HEALTH-CHECK-PASSWORD">は前述の<xref linkend="GUC-SR-CHECK-USER">、<xref linkend="GUC-SR-CHECK-PASSWORD">と同様に設定します。
   </para>
   <programlisting>
    health_check_period = 5
    # Health check period
    # Disabled (0) by default
    health_check_timeout = 30
    # Health check timeout
    # 0 means no timeout
    health_check_user = 'pgpool'
    health_check_password = ''

    health_check_max_retries = 3
   </programlisting>
   <para>
    また、バックエンド情報を前述の<literal>server1</literal>、<literal>server2</literal>及び<literal>server3</literal>の設定に従って設定しておきます。複数バックエンドノードを定義する場合、以下のbackend_*などのパラメータ名の末尾にノードIDを表す数字を付加することで複数のバックエンドを指定することができます。
   </para>
   <programlisting>
    # - Backend Connection Settings -

    backend_hostname0 = 'server1'
    # Host name or IP address to connect to for backend 0
    backend_port0 = 5432
    # Port number for backend 0
    backend_weight0 = 1
    # Weight for backend 0 (only in load balancing mode)
    backend_data_directory0 = '/var/lib/pgsql/11/data'
    # Data directory for backend 0
    backend_flag0 = 'ALLOW_TO_FAILOVER'
    # Controls various backend behavior
    # ALLOW_TO_FAILOVER or DISALLOW_TO_FAILOVER
    backend_hostname1 = 'server2'
    backend_port1 = 5432
    backend_weight1 = 1
    backend_data_directory1 = '/var/lib/pgsql/11/data'
    backend_flag1 = 'ALLOW_TO_FAILOVER'

    backend_hostname2 = 'server3'
    backend_port2 = 5432
    backend_weight2 = 1
    backend_data_directory2 = '/var/lib/pgsql/11/data'
    backend_flag2 = 'ALLOW_TO_FAILOVER'
   </programlisting>

   <para>
    <xref linkend="SQL-SHOW-POOL-NODES">コマンドでレプリケーション状態と同期レプリケーション状態を表示するには、<xref linkend="GUC-BACKEND-APPLICATION-NAME">パラメータを設定する必要があります。ここではそれぞれのホスト名を設定します。(<productname>Pgpool-II</productname> 4.1以降)
      <programlisting>
       ...                                                                          
       backend_application_name0 = 'server1'
       ...  
       backend_application_name1 = 'server2'
       ...  
       backend_application_name2 = 'server3'
      </programlisting>
   </para>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-failover">
   <title>フェイルオーバの設定</title>
   <para>
    <productname>PostgreSQL</productname>バックエンドノードがダウンした時に実行するスクリプトを<xref linkend="GUC-FAILOVER-COMMAND">に設定します。
     また、<productname>PostgreSQL</productname>サーバが3台の場合、プライマリノードのフェイルオーバ後に新しいプライマリからスレーブをリカバリするために<xref linkend="GUC-FOLLOW-MASTER-COMMAND">も設定する必要があります。<xref linkend="GUC-FOLLOW-MASTER-COMMAND">はプライマリノードのフェイルオーバ後に実行されます。<productname>PostgreSQL</productname>サーバが2台の場合、<xref linkend="GUC-FOLLOW-MASTER-COMMAND">の設定は不要です。
   </para>
   <para>
    それぞれの実行スクリプトの引数は、それぞれ実行時に<productname>Pgpool-II</productname>によってバックエンドの具体的な情報に置き換えられます。各引数の意味は<xref linkend="GUC-FAILOVER-COMMAND">をご参照ください。
   </para>
   <programlisting>
    failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
    follow_master_command = '/etc/pgpool-II/follow_master.sh %d %h %p %D %m %H %M %P %r %R'
   </programlisting>
   <note>
    <para>
     <emphasis>%N</emphasis>、<emphasis>%S</emphasis>は<productname>Pgpool-II</productname> 4.1で追加された引数です。
     <productname>Pgpool-II</productname> 4.0 または以前のバージョンを利用している場合、これらの引数を指定できないので、ご注意ください。
    </para>
   </note>
   <para>
    <filename>/etc/pgpool-II/failover.sh</filename>及び<filename>/etc/pgpool-II/follow_master.sh</filename>を作成し、実行権限を与えておきます。
   </para>
   <programlisting>
    # vi /etc/pgpool-II/failover.sh
    # vi /etc/pgpool-II/follow_master.sh
    # chmod +x /etc/pgpool-II/{failover.sh,follow_master.sh}
   </programlisting>

   <itemizedlist>
    <listitem>
     <para>
      /etc/pgpool-II/failover.sh
     </para>
     <programlisting>
#!/bin/bash
# This script is run by failover_command.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"
OLD_PRIMARY_NODE_HOST="${11}"
OLD_PRIMARY_NODE_PORT="${12}"

PGHOME=/usr/pgsql-11


logger -i -p local1.info failover.sh: start: failed_node_id=$FAILED_NODE_ID old_primary_node_id=$OLD_PRIMARY_NODE_ID failed_host=$FAILED_NODE_HOST new_master_host=$NEW_MASTER_NODE_HOST

## If there's no master node anymore, skip failover.
if [ $NEW_MASTER_NODE_ID -lt 0 ]; then
    logger -i -p local1.info failover.sh: All nodes are down. Skipping failover.
	exit 0
fi

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info failover.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## If Standby node is down, skip failover.
if [ $FAILED_NODE_ID -ne $OLD_PRIMARY_NODE_ID ]; then
    logger -i -p local1.info failover.sh: Standby node is down. Skipping failover.

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$OLD_PRIMARY_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p $OLD_PRIMARY_NODE_PORT -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error failover.sh: drop replication slot "${FAILED_NODE_HOST}" failed
        exit 1
    fi

    exit 0
fi

## Promote Standby node.
logger -i -p local1.info failover.sh: Primary node is down, promote standby node ${NEW_MASTER_NODE_HOST}.

ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote

if [ $? -ne 0 ]; then
    logger -i -p local1.error failover.sh: new_master_host=$NEW_MASTER_NODE_HOST promote failed
    exit 1
fi

logger -i -p local1.info failover.sh: end: new_master_node_id=$NEW_MASTER_NODE_ID started as the primary node
exit 0
     </programlisting>
    </listitem>
   </itemizedlist>

   <itemizedlist>
    <listitem>
     <para>
      /etc/pgpool-II/follow_master.sh
     </para>
     <programlisting>
#!/bin/bash
# This script is run after failover_command to synchronize the Standby with the new Primary.
# First try pg_rewind. If pg_rewind failed, use pg_basebackup.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPLUSER=repl
PCP_USER=pgpool
PGPOOL_PATH=/usr/bin
PCP_PORT=9898

logger -i -p local1.info follow_master.sh: start: Standby node ${FAILED_NODE_ID}

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info follow_master.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`

if [ $PGVERSION -ge 12 ]; then
RECOVERYCONF=${FAILED_NODE_PGDATA}/myrecovery.conf
else
RECOVERYCONF=${FAILED_NODE_PGDATA}/recovery.conf
fi

## Check the status of Standby
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -w -D ${FAILED_NODE_PGDATA} status


## If Standby is running, synchronize it with the new Primary.
if [ $? -eq 0 ]; then

    logger -i -p local1.info follow_master.sh: pg_rewind for $FAILED_NODE_ID

    # Create replication slot "${FAILED_NODE_HOST}"
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_create_physical_replication_slot('${FAILED_NODE_HOST}');\"
    "

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

        set -o errexit

        ${PGHOME}/bin/pg_ctl -w -m f -D ${FAILED_NODE_PGDATA} stop

        cat &gt; ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

        if [ ${PGVERSION} -ge 12 ]; then
            touch ${FAILED_NODE_PGDATA}/standby.signal
        else
            echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
        fi

        ${PGHOME}/bin/pg_rewind -D ${FAILED_NODE_PGDATA} --source-server=\"user=postgres host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT}\"

    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error follow_master.sh: end: pg_rewind failed. Try pg_basebackup.

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
             
            set -o errexit

            # Execute pg_basebackup
            rm -rf ${FAILED_NODE_PGDATA}
            rm -rf ${ARCHIVEDIR}/*
            ${PGHOME}/bin/pg_basebackup -h ${NEW_MASTER_NODE_HOST} -U $REPLUSER -p ${NEW_MASTER_NODE_PORT} -D ${FAILED_NODE_PGDATA} -X stream

            if [ ${PGVERSION} -ge 12 ]; then
                sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
                       -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${FAILED_NODE_PGDATA}/postgresql.conf
            fi
     
            cat > ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

            if [ ${PGVERSION} -ge 12 ]; then
                    touch ${FAILED_NODE_PGDATA}/standby.signal
            else
                    echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
            fi
        "

        if [ $? -ne 0 ]; then
            # drop replication slot
            ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
                ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
            "

            logger -i -p local1.error follow_master.sh: end: pg_basebackup failed
            exit 1
        fi
    fi

    # start Standby node on ${FAILED_NODE_HOST}
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool $PGHOME/bin/pg_ctl -l /dev/null -w -D ${FAILED_NODE_PGDATA} start

    # If start Standby successfully, attach this node
    if [ $? -eq 0 ]; then

        # Run pcp_attact_node to attach Standby node to Pgpool-II.
        ${PGPOOL_PATH}/pcp_attach_node -w -h localhost -U $PCP_USER -p ${PCP_PORT} -n ${FAILED_NODE_ID}

        if [ $? -ne 0 ]; then
                logger -i -p local1.error follow_master.sh: end: pcp_attach_node failed
                exit 1
        fi

    # If start Standby failed, drop replication slot "${FAILED_NODE_HOST}"
    else

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool \
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c "SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')"

        logger -i -p local1.error follow_master.sh: end: follow master command failed
        exit 1
    fi

else
    logger -i -p local1.info follow_master.sh: failed_nod_id=${FAILED_NODE_ID} is not running. skipping follow master command
    exit 0
fi

logger -i -p local1.info follow_master.sh: end: follow master command complete
exit 0
     </programlisting>
    </listitem>
   </itemizedlist>

  </sect3>

  <sect3 id="example-cluster-pgpool-config-online-recovery">
   <title>オンラインリカバリの設定</title>
   <para>
    続いて、オンラインリカバリを行うための<productname>PostgreSQL</productname>のユーザ名およびオンラインリカバリ時に呼び出されるコマンド<command>recovery_1st_stage</command>を設定します。
    オンラインリカバリで実行される<function>pgpool_recovery</function>関数は<productname>PostgreSQL</productname>のスーパーユーザ権限が必要なため、<varname>recovery_user</varname>に<emphasis>スーパーユーザ</emphasis>を指定しなければなりません。ここでは、postrgesユーザを指定します。
    オンラインリカバリ用のスクリプト<filename>recovery_1st_stage</filename>、<filename>pgpool_remote_start</filename>をプライマリサーバ(server1)のデータベースクラスタ配下に配置し、実行権限を与えておきます。
   </para>
   <programlisting>
    recovery_user = 'postgres'
    # Online recovery user
    recovery_password = ''
    # Online recovery password

    recovery_1st_stage_command = 'recovery_1st_stage'
   </programlisting>
   <programlisting>
    [server1]# su - postgres
    [server1]$ vi /var/lib/pgsql/11/data/recovery_1st_stage
    [server1]$ vi /var/lib/pgsql/11/data/pgpool_remote_start
    [server1]$ chmod +x /var/lib/pgsql/11/data/{recovery_1st_stage,pgpool_remote_start}
   </programlisting>

   <itemizedlist>
    <listitem>
     <para>
      /var/lib/pgsql/11/data/recovery_1st_stage
     </para>
     <programlisting>
#!/bin/bash
# This script is executed by "recovery_1st_stage" to recovery a Standby node.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

PRIMARY_NODE_PGDATA="$1"
DEST_NODE_HOST="$2"
DEST_NODE_PGDATA="$3"
PRIMARY_NODE_PORT="$4"
DEST_NODE_ID="$5"
DEST_NODE_PORT="$6"

PRIMARY_NODE_HOST=$(hostname)
PGHOME=/usr/pgsql-11
ARCHIVEDIR=/var/lib/pgsql/archivedir
REPLUSER=repl

logger -i -p local1.info recovery_1st_stage: start: pg_basebackup for Standby node $DEST_NODE_ID

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info recovery_1st_stage: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`
if [ $PGVERSION -ge 12 ]; then
    RECOVERYCONF=${DEST_NODE_PGDATA}/myrecovery.conf
else
    RECOVERYCONF=${DEST_NODE_PGDATA}/recovery.conf
fi

## Create replication slot "${DEST_NODE_HOST}"
${PGHOME}/bin/psql -p ${PRIMARY_NODE_PORT} &lt;&lt; EOQ
SELECT pg_create_physical_replication_slot('${DEST_NODE_HOST}');
EOQ

## Execute pg_basebackup to recovery Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "

    set -o errexit

    rm -rf $DEST_NODE_PGDATA
    rm -rf $ARCHIVEDIR/*

    ${PGHOME}/bin/pg_basebackup -h $PRIMARY_NODE_HOST -U $REPLUSER -p $PRIMARY_NODE_PORT -D $DEST_NODE_PGDATA -X stream

    if [ ${PGVERSION} -ge 12 ]; then
        sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
               -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${DEST_NODE_PGDATA}/postgresql.conf
    fi

    cat &gt; ${RECOVERYCONF} &lt;&lt; EOT
primary_conninfo = 'host=${PRIMARY_NODE_HOST} port=${PRIMARY_NODE_PORT} user=${REPLUSER} application_name=${DEST_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${PRIMARY_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${DEST_NODE_HOST}'
EOT

    if [ ${PGVERSION} -ge 12 ]; then
            touch ${DEST_NODE_PGDATA}/standby.signal
    else
            echo \"standby_mode = 'on'\" &gt;&gt; ${RECOVERYCONF}
    fi

    sed -i \"s/#*port = .*/port = ${DEST_NODE_PORT}/\" ${DEST_NODE_PGDATA}/postgresql.conf
"

if [ $? -ne 0 ]; then

    ${PGHOME}/bin/psql -p ${PRIMARY_NODE_PORT} &lt;&lt; EOQ
SELECT pg_drop_replication_slot('${DEST_NODE_HOST}');
EOQ

    logger -i -p local1.error recovery_1st_stage: end: pg_basebackup failed. online recovery failed
    exit 1
fi

logger -i -p local1.info recovery_1st_stage: end: recovery_1st_stage complete
exit 0
     </programlisting>
    </listitem>
    <listitem>

     <para>
      /var/lib/pgsql/11/data/pgpool_remote_start
     </para>
     <programlisting>
#!/bin/bash
# This script is run after recovery_1st_stage to start Standby node.

set -o xtrace
exec &gt; &gt;(logger -i -p local1.info) 2&gt;&1

PGHOME=/usr/pgsql-11
DEST_NODE_HOST="$1"
DEST_NODE_PGDATA="$2"


logger -i -p local1.info pgpool_remote_start: start: remote start Standby node $DEST_NODE_HOST

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info pgpool_remote_start: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Start Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
    $PGHOME/bin/pg_ctl -l /dev/null -w -D $DEST_NODE_PGDATA start
"

if [ $? -ne 0 ]; then
    logger -i -p local1.error pgpool_remote_start: $DEST_NODE_HOST PostgreSQL start failed.
    exit 1
fi

logger -i -p local1.info pgpool_remote_start: end: $DEST_NODE_HOST PostgreSQL started successfully.
exit 0
     </programlisting>
    </listitem>
   </itemizedlist>

   <para>
    また、オンラインリカバリ機能を使用するには、<function>pgpool_recovery</function>、<function>pgpool_remote_start</function>、<function>pgpool_switch_xlog</function>という関数が必要になるので、<literal>server1</literal>のtemplate1に<function>pgpool_recovery</function>をインストールしておきます。
   </para>
   <programlisting>
    [server1]# su - postgres
    [server1]$ psql template1 -c "CREATE EXTENSION pgpool_recovery"
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-auth">
   <title>クライアント認証の設定</title>
   <para>
    <link linkend="EXAMPLE-CLUSTER-PRE-SETUP">事前設定</link>の章で、<productname>Pgpool-II</productname>と<productname>PostgreSQL</productname>の間に認証方式を<acronym>scram-sha-256</acronym>に設定しました。この設定例では、クライアントと<productname>Pgpool-II</productname>の間でも<acronym>scram-sha-256</acronym>認証方式を利用し接続するように設定します。
    <productname>Pgpool-II</productname>のクライアント認証の設定ファイルは<filename>pool_hba.conf</filename>と呼ばれ、RPMパッケージからインストールする場合、デフォルトでは<filename>/etc/pgpool-II</filename>配下にインストールされます。
    デフォルトでは<filename>pool_hba.conf</filename>による認証は無効になっているので、<filename>pgpool.conf</filename>では以下の設定をonに変更します。
   </para>
   <programlisting>
    enable_pool_hba = on
   </programlisting>
   <para>
    <filename>pool_hba.conf</filename>のフォーマットは<productname>PostgreSQL</productname>の<filename>pg_hba.conf</filename>とほとんど同じです。<literal>pgpool</literal>と<literal>postgres</literal>ユーザを<acronym>scram-sha-256</acronym>認証に設定します。
   </para>
   <programlisting>
    host    all         pgpool           0.0.0.0/0          scram-sha-256
    host    all         postgres         0.0.0.0/0          scram-sha-256
   </programlisting>
   <note>
    <para>
     <productname>Pgpool-II</productname> 4.0の場合、<filename>pgpool.conf</filename>ファイル内の<xref linkend="guc-health-check-password">、
      <xref linkend="guc-sr-check-password">、<xref linkend="guc-wd-lifecheck-password">、
	<xref linkend="guc-recovery-password">にはAES256暗号化形式、平文形式しか指定できないので、
	 ご注意ください。
    </para>
   </note>
   <para>
    <productname>Pgpool-II</productname>のクライアント認証で用いるデフォルトのパスワードファイル名はpool_passwdです。
    <literal>scram-sha-256</literal>認証を利用する場合、<productname>Pgpool-II</productname>はそれらのパスワードを復号化するために復号鍵が必要となります。全サーバで復号鍵ファイルを<productname>Pgpool-II</productname>の起動ユーザ<literal>postgres</literal>(<productname>Pgpool-II</productname> 4.1以降)のホームディレクトリ配下に作成します。(<productname>Pgpool-II</productname> 4.0 以前のバージョンでは、デフォルトでは<literal>root</literal>ユーザで<productname>Pgpool-II</productname>を起動する)
   </para>
   <programlisting>
    [全サーバ]# su - postgres
    [全サーバ]$ echo '任意の文字列' > ~/.pgpoolkey 
    [全サーバ]$ chmod 600 ~/.pgpoolkey
   </programlisting>
   <para>
    「pg_enc -m -k /path/to/.pgpoolkey -u ユーザ名 -p」 コマンドを実行すると、ユーザ名と<literal>AES256</literal>で暗号化したパスワードのエントリが<xref linkend="GUC-POOL-PASSWD">に登録されます。 
     <xref linkend="GUC-POOL-PASSWD"> がまだ存在しなければ、<filename>pgpool.conf</filename>と同じディレクトリ内に作成されます。
   </para>
   <programlisting>
    [全サーバ]# su - postgres
    [全サーバ]$ pg_enc -m -k ~/.pgpoolkey -u pgpool -p
    db password: [pgpoolユーザのパスワード]
    [全サーバ]$ pg_enc -m -k ~/.pgpoolkey -u postgres -p
    db password: [postgresユーザのパスワード]

    # cat /etc/pgpool-II/pool_passwd 
    pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
    postgres:AESHs/pWL5rtXy2IwuzroHfqg==
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-watchdog">
   <title>Watchdogの設定</title>
   <para>
    デフォルトでは<literal>watchdog</literal>機能が無効のため、<literal>server1</literal>、<literal>server2</literal>及び<literal>server3</literal>で<literal>watchdog</literal>を有効にします。
   </para>
   <programlisting>
    use_watchdog = on
   </programlisting>
   <para>
    アクティブ機が立ち上げる仮想IPをdelegate_IPに指定します。仮想 IP はまだ使われていないIPアドレスを指定してください。<literal>server1</literal>、<literal>server2</literal>及び<literal>server3</literal>の共通の設定です。
   </para>
   <programlisting>
    delegate_IP = '192.168.137.150'
   </programlisting>

   <para>
    仮想IPの起動/停止、ARPリクエストの送信を行う設定パラメータ<xref linkend="GUC-IF-UP-CMD">、<xref linkend="GUC-IF-DOWN-CMD">、<xref linkend="GUC-ARPING-CMD">に、ネットワーク環境に合わせてネットワークインターフェース名を設定します。
    今回の例で使ったネットワークインターフェースは「enp0s8」となっています。
    <varname>if_up/down_cmd</varname>や<varname>arping_cmd</varname>を実行するにはroot権限が必要となりますので、
    一般ユーザが実行できるように<command>ip/arping</command>コマンドに<literal>setuid</literal>を設定するか、
    <productname>Pgpool-II</productname>起動ユーザ、デフォルトでは<literal>postgres</literal>ユーザ (<productname>Pgpool-II</productname> 4.1以降) がパスワードなしに<command>sudo</command>を実行できるように設定する必要があります。
    ここでは、<command>sudo</command>を介して実行するように設定します。RPMからインストールした場合、<literal>postgres</literal>ユーザがパスワードなしに<command>sudo</command>を介して<command>ip/arping</command>を実行できるように設定済みです。
   </para>
   <programlisting>
if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev enp0s8'
arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I enp0s8'
   </programlisting>
   <note>
    <para>
     <filename>/etc/sudoers</filename>で「Defaults requiretty」を設定している場合は、<productname>pgpool</productname>の起動ユーザが<literal>tty</literal>なしで<command>if_up_cmd</command>、
     <command>if_down_cmd</command>及び<command>arping_cmd</command>コマンドを実行できるように設定する必要があります。 
    </para>
   </note>

   <para>
	ipコマンドやarpingコマンドのパスがデフォルトのパスと異なる場合、環境に合わせて<xref linkend="GUC-IF-CMD-PATH">や<xref linkend="GUC-ARPING-PATH">を設定しておいてください。
    ただし、 <varname>if_up/down_cmd</varname>及び<varname>arping_cmd</varname>に指定したコマンドが"/"で始まる場合、 フルパスとみなし<varname>if_cmd_path</varname>及び<varname>arping_path</varname>の設定を無視します。
   </para>
   <programlisting>
if_cmd_path = '/sbin'
arping_path = '/usr/sbin'
   </programlisting>
   <para>
    各watchdog が稼働するサーバ情報を設定しておきます。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>server1</literal>の場合
     </para>
     <programlisting>
      wd_hostname = 'server1'
      wd_port = 9000
     </programlisting>
    </listitem>
    <listitem>
     <para>
      <literal>server2</literal>の場合
     </para>
     <programlisting>
      wd_hostname = 'server2'
      wd_port = 9000
     </programlisting>
    </listitem>
    <listitem>
     <para>
      <literal>server3</literal>の場合
     </para>
     <programlisting>
      wd_hostname = 'server3'
      wd_port = 9000
     </programlisting>
    </listitem>
   </itemizedlist>

   <para>
    各監視対象の<productname>Pgpool-II</productname>サーバ情報を設定しておきます。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>server1</literal>の場合
     </para>
     <programlisting>
      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server2'
      # Host name or IP address to connect to for other pgpool 0
      # (change requires restart)
      other_pgpool_port0 = 9999
      # Port number for other pgpool 0
      # (change requires restart)
      other_wd_port0 = 9000
      # Port number for other watchdog 0
      # (change requires restart)
      other_pgpool_hostname1 = 'server3'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </programlisting>
    </listitem>
    <listitem>
     <para>
      <literal>server2</literal>の場合
     </para>
     <programlisting>
      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server1'
      # Host name or IP address to connect to for other pgpool 0
      # (change requires restart)
      other_pgpool_port0 = 9999
      # Port number for other pgpool 0
      # (change requires restart)
      other_wd_port0 = 9000
      # Port number for other watchdog 0
      # (change requires restart)
      other_pgpool_hostname1 = 'server3'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </programlisting>
    </listitem>
    <listitem>
     <para>
      <literal>server3</literal>の場合
     </para>
     <programlisting>
      # - Other pgpool Connection Settings -

      other_pgpool_hostname0 = 'server1'
      # Host name or IP address to connect to for other pgpool 0
      # (change requires restart)
      other_pgpool_port0 = 9999
      # Port number for other pgpool 0
      # (change requires restart)
      other_wd_port0 = 9000
      # Port number for other watchdog 0
      # (change requires restart)
      other_pgpool_hostname1 = 'server2'
      other_pgpool_port1 = 9999
      other_wd_port1 = 9000
     </programlisting>
    </listitem>
   </itemizedlist>

   <para>
    ハートビート信号の送信先のホスト名とポート番号を指定します。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>server1</literal>の場合
     </para>
     <programlisting>
      heartbeat_destination0 = 'server2'
      # Host name or IP address of destination 0
      # for sending heartbeat signal.
      # (change requires restart)
      heartbeat_destination_port0 = 9694
      # Port number of destination 0 for sending
      # heartbeat signal. Usually this is the
      # same as wd_heartbeat_port.
      # (change requires restart)
      heartbeat_device0 = ''
      # Name of NIC device (such like 'eth0')
      # used for sending/receiving heartbeat
      # signal to/from destination 0.
      # This works only when this is not empty
      # and pgpool has root privilege.
      # (change requires restart)

      heartbeat_destination1 = 'server3'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''

     </programlisting>
    </listitem>
    <listitem>
     <para>
      <literal>server2</literal>の場合
     </para>
     <programlisting>
      heartbeat_destination0 = 'server1'
      # Host name or IP address of destination 0
      # for sending heartbeat signal.
      # (change requires restart)
      heartbeat_destination_port0 = 9694
      # Port number of destination 0 for sending
      # heartbeat signal. Usually this is the
      # same as wd_heartbeat_port.
      # (change requires restart)
      heartbeat_device0 = ''
      # Name of NIC device (such like 'eth0')
      # used for sending/receiving heartbeat
      # signal to/from destination 0.
      # This works only when this is not empty
      # and pgpool has root privilege.
      # (change requires restart)

      heartbeat_destination1 = 'server3'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''

     </programlisting>
    </listitem>
    <listitem>
     <para>
      <literal>server3</literal>の場合
     </para>
     <programlisting>
      heartbeat_destination0 = 'server1'
      # Host name or IP address of destination 0
      # for sending heartbeat signal.
      # (change requires restart)
      heartbeat_destination_port0 = 9694
      # Port number of destination 0 for sending
      # heartbeat signal. Usually this is the
      # same as wd_heartbeat_port.
      # (change requires restart)
      heartbeat_device0 = ''
      # Name of NIC device (such like 'eth0')
      # used for sending/receiving heartbeat
      # signal to/from destination 0.
      # This works only when this is not empty
      # and pgpool has root privilege.
      # (change requires restart)

      heartbeat_destination1 = 'server2'
      heartbeat_destination_port1 = 9694
      heartbeat_device1 = ''
     </programlisting>
    </listitem>
   </itemizedlist>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-sysconfig">
   <title>/etc/sysconfig/pgpoolの設定</title>
   <para>
    <productname>Pgpool-II</productname>を起動時に<filename>pgpool_status</filename>ファイルを無視させたい場合、<filename>/etc/sysconfig/pgpool</filename>の起動オプションOPTSに「-D」を追加します。
   </para>
   <programlisting>
    [全サーバ]# vi /etc/sysconfig/pgpool 
    (...省略...)
    OPTS=" -D -n"
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-log">
   <title>ログの設定</title>
   <para>
    この例では、<productname>Pgpool-II</productname>のログ出力は<literal>syslog</literal>を利用するように設定します。
   </para>
   <programlisting>
    log_destination = 'syslog'
    # Where to log
    # Valid values are combinations of stderr,
    # and syslog. Default to stderr.

    syslog_facility = 'LOCAL1'
    # Syslog local facility. Default to LOCAL0
   </programlisting>
   <para>
    全サーバではログファイルを作成します。
   </para>
   <programlisting>
    [全サーバ]# mkdir /var/log/pgpool-II
    [全サーバ]# touch /var/log/pgpool-II/pgpool.log
   </programlisting>
   <para>
    次に<literal>syslog</literal>の設定ファイルを以下のように編集します。
   </para>
   <programlisting>
    [全サーバ]# vi /etc/rsyslog.conf
    ...(省略)...
    *.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
    LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
   </programlisting>
   <para>
    また、<productname>Pgpool-II</productname>に関して<filename>/var/log/messages</filename>と同様のログローテーションを行うように、logrotateの設定を以下のように行います。
   </para>
   <programlisting>
    [全サーバ]# vi /etc/logrotate.d/syslog
    ...(省略)...
    /var/log/messages
    /var/log/pgpool-II/pgpool.log
    /var/log/secure
   </programlisting>

   <para>
    設定が終わったら、rsyslogサービスを再起動します。
   </para>
   <programlisting>
    [全サーバ]# systemctl restart rsyslog
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-pcp">
   <title>PCPコマンドの設定</title>
   <para>
    <literal>PCP</literal>コマンドを使用するにはユーザ認証が必要になるので、ユーザ名と<literal>md5</literal>ハッシュに変換されたパスワードを<filename>pcp.conf</filename>ファイルに設定します。
    ここではユーザ名に<literal>pgpool</literal>を使用し、以下のコマンドを実行することで、&lt;ユーザ名:ハッシュ化されたパスワード&gt;が<filename>/etc/pgpool-II/pcp.conf</filename>に追加されます。
   </para>
   <programlisting>
    [全サーバ]# echo 'pgpool:'`pg_md5 PCPコマンドパスワード` &gt;&gt; /etc/pgpool-II/pcp.conf
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-pgpool-config-pcppass">
   <title>.pcppassの設定</title>
   <para>
    前述の<literal>follow_master_command</literal>のスクリプトでパスワード入力なしで<literal>PCP</literal>コマンドを実行できるように、すべてのサーバで<productname>Pgpool-II</productname>の起動ユーザのホームディレクトリに<filename>.pcppass</filename>を作成します。
   </para>
   <programlisting>
    [全サーバ]# echo 'localhost:9898:pgpool:pgpool' > ~/.pcppass
    [全サーバ]# chmod 600 ~/.pcppass
   </programlisting>
   <para>
    ここで、<productname>Pgpool-II</productname>の設定は完了です。
   </para>
  </sect3>
 </sect2>

 <sect2 id="example-cluster-start-stop">
  <title>システムの起動と停止</title>
  <para>
   <productname>Pgpool-II</productname>の設定が完了したら、次に<productname>Pgpool-II</productname>を起動します。<productname>Pgpool-II</productname>を起動する前に、バックエンドの<productname>PostgreSQL</productname>をあらかじめ起動する必要があります。また、<productname>PostgreSQL</productname>を停止する場合、<productname>Pgpool-II</productname>を先に停止する必要があります。
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <productname>Pgpool-II</productname>の起動
    </para>
    <para>
     前述の<link linkend="EXAMPLE-CLUSTER-PRE-SETUP">事前設定</link>の章で<productname>Pgpool-II</productname>の自動起動が設定済なので、ここでシステム全体を再起動するか、以下のコマンドを実行してください。
    </para>
    <programlisting>
     # systemctl start pgpool.service
    </programlisting>
   </listitem>
   <listitem>
    <para>
     <productname>Pgpool-II</productname>の停止
    </para>
    <programlisting>
     # systemctl stop pgpool.service
    </programlisting>
   </listitem>
  </itemizedlist>
 </sect2>

 <sect2 id="example-cluster-try">
  <title>動作確認</title>
  <para>
   これから、動作確認を行います。まず、<literal>server1</literal>、<literal>server2</literal>、<literal>server3</literal>で以下のコマンドで<productname>Pgpool-II</productname>を起動します。
  </para>
  <programlisting>
   # systemctl start pgpool.service
  </programlisting>

  <sect3 id="example-cluster-try-standby">
   <title>PostgreSQL スタンバイサーバを構築</title>
   <para>
    まず、<productname>Pgpool-II</productname>のオンラインリカバリ機能を利用し、スタンバイサーバを構築します。<command>pcp_recovery_node</command>コマンドで実行される<varname>recovery_1st_stage_command</varname>パラメータに指定した<filename>recovery_1st_stage</filename>と<filename>pgpool_remote_start</filename>スプリクトが実行されるので、この 2つのスクリプトが現在稼働中のプライマリサーバ<literal>server1</literal>のデータベースクラスタの下に存在することを確認します。
   </para>
   <programlisting>
    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 1
    Password: 
    pcp_recovery_node -- Command Successful

    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 2
    Password: 
    pcp_recovery_node -- Command Successful
   </programlisting>
   <para>
    <literal>server2</literal>と<literal>server3</literal>の<productname>PostgreSQL</productname>がスタンバイとして起動されていることを確認します。
   </para>
   <programlisting>
    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
    1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
    (3 rows)
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-try-watchdog">
   <title>watchdogアクティブ/スタンバイの切り替え</title>
   <para>
    <command>pcp_watchdog_info</command>で<productname>Pgpool-II</productname>の<application>watchdog</application>の情報を確認します。最初に起動した<productname>Pgpool-II</productname>が「MASTER」になります。
   </para>
   <programlisting>
    # pcp_watchdog_info -h 192.168.137.150 -p 9898 -U pgpool
    Password: 
    3 YES server1:9999 Linux server1 server1

    server1:9999 Linux server1 server1 9999 9000 4 MASTER  #最初に起動されたサーバがMASTERになる
    server2:9999 Linux server2 server2 9999 9000 7 STANDBY #スタンバイとして稼働
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY #スタンバイとして稼働
   </programlisting>
   <para>
    アクティブである<literal>server1</literal>の<productname>Pgpool-II</productname>を停止し、<literal>server2</literal>または<literal>server3</literal>がスタンバイからアクティブに昇格することを確認します。<literal>server1</literal>を停止する方法は<productname>Pgpool-II</productname>を停止する、またはマシンをシャットダウンします。ここでは、<productname>Pgpool-II</productname>を停止します。
   </para>
   <programlisting>
    [server1]# systemctl stop pgpool.service

    # pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
    Password: 
    3 YES server2:9999 Linux server2 server2

    server2:9999 Linux server2 server2 9999 9000 4 MASTER     #server2がアクティブに昇格
    server1:9999 Linux server1 server1 9999 9000 10 SHUTDOWN  #server1が停止された
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY    #スタンバイとして稼働
   </programlisting>
   <para>
    先ほど停止した<productname>Pgpool-II</productname>を再起動し、スタンバイとして起動したことを確認します。
   </para>
   <programlisting>
    [server1]# systemctl start pgpool.service

    [server1]# pcp_watchdog_info -p 9898 -h 192.168.137.150 -U pgpool
    Password: 
    3 YES server2:9999 Linux server2 server2

    server2:9999 Linux server2 server2 9999 9000 4 MASTER
    server1:9999 Linux server1 server1 9999 9000 7 STANDBY
    server3:9999 Linux server3 server3 9999 9000 7 STANDBY
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-try-failover">
   <title>自動フェイルオーバ</title>
   <para>
    <command>psql</command>で仮想IPに接続し、バックエンドの情報を確認します。
   </para>
   <programlisting>
    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:13:17
    1       | server2  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:13:25
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:14:20
    (3 rows)
   </programlisting>
   <para>
    次にプライマリである<literal>server1</literal>の<productname>PostgreSQL</productname>を停止し、フェイルオーバするかどうか確認してみます。
   </para>
   <programlisting>
    [server1]$ pg_ctl -D /var/lib/pgsql/11/data -m immediate stop
   </programlisting>
   <para>
    <literal>ノード1</literal>を停止後、フェイルオーバが発生し、<literal>server2</literal>がプライマリに昇格したことを確認します。
   </para>
   <programlisting>
    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
    1       | server2  | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 |                   |                        | 2019-08-06 11:36:03
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:36:15
    (3 rows)
   </programlisting>

   <para>
    <literal>server3</literal>が新しいプライマリ<literal>server2</literal>のスタンバイとして起動されています。
   </para>

   <programlisting>
    [server3]# psql -h server3 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    t

    [server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    f

    [server2]# psql -h server2 -p 5432 -U pgpool postgres -c "select * from pg_stat_replication" -x
    -[ RECORD 1 ]----+------------------------------
    pid              | 11059
    usesysid         | 16392
    usename          | repl
    application_name | server3
    client_addr      | 192.168.137.103
    client_hostname  | 
    client_port      | 48694
    backend_start    | 2019-08-06 11:36:07.479161+09
    backend_xmin     | 
    state            | streaming
    sent_lsn         | 0/75000148
    write_lsn        | 0/75000148
    flush_lsn        | 0/75000148
    replay_lsn       | 0/75000148
    write_lag        | 
    flush_lag        | 
    replay_lag       | 
    sync_priority    | 0
    sync_state       | async
    reply_time       | 2019-08-06 11:42:59.823961+09
   </programlisting>
  </sect3>

  <sect3 id="example-cluster-try-online-recovery">
   <title>オンラインリカバリ</title>
   <para>
    次に、<productname>Pgpool-II</productname>のオンラインリカバリ機能を利用し、先ほど停止した旧プライマリサーバをスタンバイとして復旧させます。<command>pcp_recovery_node</command>コマンドで実行される<varname>recovery_1st_stage_command</varname>パラメータに指定した<filename>recovery_1st_stage</filename>と<filename>pgpool_remote_start</filename>スプリクトが現在稼働中のプライマリサーバ<literal>server2</literal>のデータベースクラスタの下に存在することを確認します。
   </para>
   <programlisting>
    # pcp_recovery_node -h 192.168.137.150 -p 9898 -U pgpool -n 0
    Password: 
    pcp_recovery_node -- Command Successful
   </programlisting>
   <para>
    <literal>ノード1</literal>がスタンバイとして起動されたことを確認します。
   </para>
   <programlisting>
    # psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
    node_id | hostname | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state | last_status_change  
    ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+---------------------
    0       | server1  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  | 2019-08-06 11:48:05
    1       | server2  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        | 2019-08-06 11:36:03
    2       | server3  | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  | 2019-08-06 11:36:15
    (3 rows)
   </programlisting>
   <para>
    以上で、動作確認が完了です。
   </para>
  </sect3>
 </sect2>
</sect1>
